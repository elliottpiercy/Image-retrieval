{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import WTA_CONV\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', validation_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train config class\n",
    "class train_config:\n",
    "    \n",
    "    epochs = 20\n",
    "    batch_size = 300\n",
    "    learning_rate = 0.001\n",
    "    save_path = 'models/'\n",
    "    save_name='model.cptk'\n",
    "\n",
    "\n",
    "# Test config class     \n",
    "class test_config:\n",
    "    num_test_imgs = 300\n",
    "    save_path = 'models/'\n",
    "    save_name='model.cptk'\n",
    "    \n",
    "\n",
    "\n",
    "train_config = train_config()\n",
    "test_config = test_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "##### Training below this line #####\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sparcity = [0.05,0.2,0.4,0.6,0.8,0.95]\n",
    "\n",
    "for sparse_const in sparcity:\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    model = WTA_CONV.WTA_CONV(mnist.train, mnist.validation, mnist.test)\n",
    "\n",
    "    inputs = tf.placeholder(tf.float32, (None, 28, 28, 1), name='inputs')\n",
    "    targets = tf.placeholder(tf.float32, (None, 28, 28, 1), name='targets')\n",
    "\n",
    "    WTA_CONV_model = model.model(inputs,train_config,reuse = False)\n",
    "\n",
    "\n",
    "    loss = model.loss(targets = targets, model = WTA_CONV_model, lifetime_sparsity = sparse_const)\n",
    "    opt = tf.train.AdamOptimizer(train_config.learning_rate).minimize(loss)\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for e in range(train_config.epochs):\n",
    "\n",
    "        for ii in range(0,model.train.num_examples // train_config.batch_size, 2):\n",
    "            batch = model.train.next_batch(train_config.batch_size)\n",
    "            imgs = batch[0].reshape((-1, 28, 28, 1))\n",
    "\n",
    "\n",
    "\n",
    "            batch_cost, _ = sess.run([loss, opt], feed_dict={inputs: imgs,\n",
    "                                                             targets: imgs})\n",
    "\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, train_config.epochs),\n",
    "                  \"Training loss: {:.4f}\".format(batch_cost))\n",
    "            \n",
    "        saver = tf.train.Saver()\n",
    "        name = 'model_' + str(e) + '_sparcity_' + str(sparse_const) + '.cptk'\n",
    "        save_path = saver.save(sess, os.path.join(train_config.save_path, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Above this line is training ### \n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initiate model and placeholders\n",
    "# Array of the best models for each sparcity constant\n",
    "best_models = np.array([])\n",
    "best_accuracys = np.array([])\n",
    "\n",
    "for sparse_const in sparcity:\n",
    "    print(\"--------   \",\"Constant:\",sparse_const,\"   --------------\")\n",
    "    max_accuracy = 0\n",
    "    best_model = -1\n",
    "    best_positions = None\n",
    "    \n",
    "    for e in range(15):\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        test_config.save_name = 'model_' + str(e) + '_sparcity_' + str(sparse_const) + '.cptk'\n",
    "        \n",
    "        model = WTA_CONV.WTA_CONV(mnist.train, mnist.validation, mnist.test)\n",
    "\n",
    "\n",
    "        inputs = tf.placeholder(tf.float32, (None, 28, 28, 1), name='inputs')\n",
    "        targets = tf.placeholder(tf.float32, (None, 28, 28, 1), name='targets')\n",
    "\n",
    "        WTA_CONV_model = model.model(inputs,train_config,False)\n",
    "\n",
    "        loss = model.loss(targets, WTA_CONV_model, 0.2)\n",
    "\n",
    "        # Restore model\n",
    "        sess = tf.Session()\n",
    "        saver = tf.train.Saver()\n",
    "        saver = saver.restore(sess, os.path.join(test_config.save_path,test_config.save_name))\n",
    "\n",
    "        # Get test data\n",
    "        imgs = np.reshape(model.validation.images[:test_config.num_test_imgs],(-1,28,28,1))\n",
    "\n",
    "        # Get embeddings\n",
    "        get_embeddings = model.get_embedding(WTA_CONV_model)\n",
    "\n",
    "\n",
    "        embeddings= sess.run([get_embeddings], feed_dict={inputs: imgs,\n",
    "                                                          targets: imgs})\n",
    "\n",
    "\n",
    "\n",
    "        # Embeddings returned from test data\n",
    "        embeddings = np.reshape(embeddings,(test_config.num_test_imgs,4,4,8))\n",
    "\n",
    "\n",
    "\n",
    "        predicted_labels = []\n",
    "        test_config.num_test_imgs = 300\n",
    "        neighbours = 10\n",
    "\n",
    "        classifier = model.fit_KNN(embeddings, neighbours, p=2)\n",
    "        print('Classifier fit. Finding nearest neighbours.')\n",
    "\n",
    "        for query in range(test_config.num_test_imgs):\n",
    "            indices = model.K_nearest_neighbours(classifier, embeddings[query])\n",
    "            predicted_labels = np.append(predicted_labels,indices)\n",
    "\n",
    "        predicted_labels = np.reshape(predicted_labels,(test_config.num_test_imgs, neighbours))\n",
    "\n",
    "        accuracy, positions = model.model_accuracy(predicted_labels, test_config.num_test_imgs, neighbours)\n",
    "\n",
    "        if accuracy > max_accuracy:\n",
    "            best_model = test_config.save_name\n",
    "            max_accuracy = accuracy\n",
    "            best_positions = positions\n",
    "            \n",
    "    best_models = np.append(best_models,test_config.save_name)\n",
    "    best_accuracys = np.append(best_accuracys,max_accuracy)\n",
    "    model.plot_bar_chart(best_positions, test_config.num_test_imgs)\n",
    "    print(\"Best model:\",best_model)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(best_models)\n",
    "print(best_accuracys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing accuracy of feature diffusion\n",
    "\n",
    "embed = []\n",
    "\n",
    "for name in best_models[:4]:\n",
    "    tf.reset_default_graph()\n",
    "    test_config.save_name = name\n",
    "\n",
    "    model = WTA_CONV.WTA_CONV(mnist.train, mnist.validation, mnist.test)\n",
    "\n",
    "\n",
    "    inputs = tf.placeholder(tf.float32, (None, 28, 28, 1), name='inputs')\n",
    "    targets = tf.placeholder(tf.float32, (None, 28, 28, 1), name='targets')\n",
    "\n",
    "    WTA_CONV_model = model.model(inputs,train_config,False)\n",
    "\n",
    "    loss = model.loss(targets, WTA_CONV_model, 0.2)\n",
    "\n",
    "    # Restore model\n",
    "    sess = tf.Session()\n",
    "    saver = tf.train.Saver()\n",
    "    saver = saver.restore(sess, os.path.join(test_config.save_path,test_config.save_name))\n",
    "\n",
    "    # Get test data\n",
    "    imgs = np.reshape(model.validation.images[:test_config.num_test_imgs],(-1,28,28,1))\n",
    "\n",
    "    # Get embeddings\n",
    "    get_embeddings = model.get_embedding(WTA_CONV_model)\n",
    "\n",
    "\n",
    "    embeddings= sess.run([get_embeddings], feed_dict={inputs: imgs,\n",
    "                                                      targets: imgs})\n",
    "\n",
    "\n",
    "\n",
    "    # Embeddings returned from test data\n",
    "    embeddings = np.reshape(embeddings,(test_config.num_test_imgs,4,4,8))\n",
    "    embeddings = model.flatten_embeddings(embeddings)\n",
    "    \n",
    "    embed = np.append(embed,embeddings)\n",
    "embed = np.reshape(embed,(4,test_config.num_test_imgs, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Collaborative_Affinity_Metric_Fusion\n",
    "features_algo_1 = embed[0]\n",
    "features_algo_2 = embed[1]\n",
    "features_algo_3 = embed[2]\n",
    "features_algo_4 = embed[3]\n",
    "\n",
    "\n",
    "CAMF = Collaborative_Affinity_Metric_Fusion.collaborative_affinity_metric_fusion()\n",
    "\n",
    "status_matrix_1, kernel_matrix_1 = CAMF.create_graphs(features_algo_1, neighbours = 20)\n",
    "status_matrix_2, kernel_matrix_2 = CAMF.create_graphs(features_algo_2, neighbours = 20)\n",
    "status_matrix_3, kernel_matrix_3 = CAMF.create_graphs(features_algo_3, neighbours = 20)\n",
    "status_matrix_4, kernel_matrix_4 = CAMF.create_graphs(features_algo_4, neighbours = 20)\n",
    "\n",
    "\n",
    "status_matrix = CAMF.merge_graph(status_matrix_1, status_matrix_2, status_matrix_3, status_matrix_4)\n",
    "kernel_matrix = CAMF.merge_graph(kernel_matrix_1, kernel_matrix_2, kernel_matrix_3, kernel_matrix_4)\n",
    "W_FAM = CAMF.cross_diffusion(status_matrix, kernel_matrix)\n",
    "similarity_matrix = CAMF.image_similarity(W_FAM,10)\n",
    "\n",
    "predicted_labels = []\n",
    "true_labels = []\n",
    "\n",
    "for row in similarity_matrix:\n",
    "    for col in row:\n",
    "        label = mnist.validation.labels[int(col)]\n",
    "        predicted_labels = np.append(predicted_labels, label)\n",
    "\n",
    "predicted_labels = np.reshape(predicted_labels,similarity_matrix.shape)\n",
    "\n",
    "for i in range(test_config.num_test_imgs):\n",
    "    true_labels = np.append(true_labels,mnist.validation.images[int(i)])\n",
    "\n",
    "accuracy, positions = model.model_accuracy(predicted_labels, 300, 10)\n",
    "model.plot_bar_chart(positions, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
